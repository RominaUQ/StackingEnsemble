{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start by getting our SM environemnt ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "bucket = 'stacking-ensemble'\n",
    "#prefix = 'sagemaker/DEMO-stacking-ensemble'\n",
    " \n",
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "region=boto3.Session().region_name\n",
    "account=boto3.client('sts').get_caller_identity()['Account']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's bring in the Python libraries that we'll use throughout the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                # For matrix operations and numerical processing\n",
    "import pandas as pd                               # For munging tabular data\n",
    "import matplotlib.pyplot as plt                   # For charts and visualizations\n",
    "from IPython.display import Image                 # For displaying images in the notebook\n",
    "from IPython.display import display               # For displaying outputs in the notebook\n",
    "from time import gmtime, strftime                 # For labeling SageMaker models, endpoints, etc.\n",
    "import sys                                        # For writing outputs to notebook\n",
    "import math                                       # For ceiling function\n",
    "import json                                       # For parsing hosting outputs\n",
    "import os                                         # For manipulating filepath names\n",
    "import sagemaker                                  # Amazon SageMaker's Python SDK provides many helper functions\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Binarizer, StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets read our data from our S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_key = 'UCI_Credit_Card.csv'\n",
    "data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "\n",
    "data=pd.read_csv(data_location)\n",
    "#data.info()\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets read this into a Pandas data frame and take a look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration\n",
    "Let's start exploring the data.  First, let's understand how the features are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency tables for each categorical feature\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    display(pd.crosstab(index=data[column], columns='% observations', normalize='columns'))\n",
    "\n",
    "# Histograms for each numeric features\n",
    "display(data.describe())\n",
    "%matplotlib inline\n",
    "hist = data.hist(bins=30, sharey=True, figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = data.drop(['ID', 'default.payment.next.month'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardise our numerical features and one hot encode our categorial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features= data[['LIMIT_BAL', 'PAY_0',\n",
    "       'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2',\n",
    "       'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1',\n",
    "       'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']]\n",
    "\n",
    "numeric_transformer= StandardScaler()\n",
    "num_array=numeric_transformer.fit_transform(numeric_features)\n",
    "print(num_array.shape)\n",
    "\n",
    "\n",
    "categorical_features = data[['SEX', 'EDUCATION', 'MARRIAGE', 'AGE']]\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore').fit(categorical_features)\n",
    "cat_array=categorical_transformer.transform(categorical_features).toarray()\n",
    "print(cat_array.shape)\n",
    "\n",
    "processed_array=np.concatenate((num_array, cat_array), 1)\n",
    "print(processed_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_matirx= np.asmatrix(processed_array)\n",
    "X_data=pd.DataFrame(processed_matirx)\n",
    "Y_data = data[['default.payment.next.month']]\n",
    "X_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split into train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, test_size=0.30, random_state=42)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([Y_train, X_train], axis=1).to_csv('train.csv', index=False, header=False)\n",
    "pd.concat([Y_val, X_val], axis=1).to_csv('validation.csv', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join('train/train.csv')).upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join('validation/validation.csv')).upload_file('validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.inputs.TrainingInput(s3_data='s3://{}/train'.format(bucket), content_type='text/csv')\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(s3_data='s3://{}/validation/'.format(bucket), content_type='text/csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_1 = sagemaker.image_uris.retrieve(region=boto3.Session().region_name, framework='xgboost', version='latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container_1,\n",
    "                                    role, \n",
    "                                    instance_count=1, \n",
    "                                    instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/output'.format(bucket),\n",
    "                                    sagemaker_session=sess)\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100)\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model_url= xgb.model_data\n",
    "print(xgb_model_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Linear learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container_2 = sagemaker.image_uris.retrieve(region=boto3.Session().region_name, framework='linear-learner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = sagemaker.estimator.Estimator(\n",
    "    container_2,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c4.xlarge\",\n",
    "    output_path='s3://{}/output'.format(bucket),\n",
    "    sagemaker_session=sess,\n",
    "    content_type='text/csv'\n",
    ")\n",
    "linear.set_hyperparameters(feature_dim=88, predictor_type=\"binary_classifier\", mini_batch_size=200)\n",
    "\n",
    "linear.fit({'train': s3_input_train, 'validation': s3_input_validation}) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model_url=linear.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy a multi container endpoint that can host both our learner and later we will add our meta learner model to it as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container1 = { 'Image': container_1,\n",
    "                'ContainerHostname': 'xgbContainer',\n",
    "             'ModelDataUrl': xgb_model_url}\n",
    "\n",
    "container2 = { 'Image': container_2,\n",
    "                'ContainerHostname': 'LinearlearnerContainer',\n",
    "              'ModelDataUrl':linear_model_url\n",
    "             }\n",
    "inferenceExecutionConfig = {'Mode': 'Direct'}    \n",
    "\n",
    "sm_client = boto3.Session().client('sagemaker')\n",
    "\n",
    "model_name= 'my-direct-model' + strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "response = sm_client.create_model(ModelName =model_name,\n",
    "              InferenceExecutionConfig = inferenceExecutionConfig,\n",
    "              ExecutionRoleArn = role,\n",
    "              Containers = [container1, container2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = 'my-epc',\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':        'ml.m4.xlarge',\n",
    "        'InitialInstanceCount': 2,\n",
    "        'InitialVariantWeight': 1,\n",
    "        'ModelName':            model_name,\n",
    "        'VariantName':          'AllTraffic'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm_client.create_endpoint(\n",
    "              EndpointName       = 'my-endpoint',\n",
    "              EndpointConfigName = 'my-epc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke the endpoint to make predictions using each of the models we deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "runtime_sm_client = boto3.Session().client('sagemaker-runtime')\n",
    "import io\n",
    "from io import StringIO\n",
    "csv_file = io.StringIO()\n",
    "results_XGB=list()\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    body=X_test.iloc[[i]]\n",
    "    csv_file=io.StringIO()\n",
    "    body.to_csv(csv_file, sep=\",\", header=False, index=False)\n",
    "    payload = csv_file.getvalue()\n",
    "    response = runtime_sm_client.invoke_endpoint(EndpointName ='my-endpoint',ContentType = 'text/csv',TargetContainerHostname='xgbContainer', Body = payload)\n",
    "    result = json.loads(response['Body'].read().decode())\n",
    "    results_XGB.append(result)\n",
    "#print(results_XGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate confusion matrix and calculate accuracy for our XGB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test=Y_test.to_numpy()\n",
    "Y_test=Y_test.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_xgb=pd.crosstab(index=Y_test, columns=np.round(results_XGB), rownames=['actuals'], colnames=['predictions'])\n",
    "print(cm_xgb)\n",
    "\n",
    "(cm_xgb.iloc[0,0]+cm_xgb.iloc[1,1])/len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "runtime_sm_client = boto3.Session().client('sagemaker-runtime')\n",
    "import io\n",
    "from io import StringIO\n",
    "csv_file = io.StringIO()\n",
    "results_LL=list()\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    body=X_test.iloc[[i]]\n",
    "    csv_file=io.StringIO()\n",
    "    body.to_csv(csv_file, sep=\",\", header=False, index=False)\n",
    "    payload = csv_file.getvalue()\n",
    "    response = runtime_sm_client.invoke_endpoint(EndpointName ='my-endpoint',ContentType = 'text/csv',TargetContainerHostname='LinearlearnerContainer', Body = payload)\n",
    "    result = json.loads(response['Body'].read().decode())\n",
    "    result=result['predictions'][0]['score']\n",
    "    results_LL.append(result)\n",
    "#print(results_LL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate confusion matrix and calculate accuracy for our Linear learner model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_ll=pd.crosstab(index=Y_test, columns=np.round(results_LL), rownames=['actuals'], colnames=['predictions'])\n",
    "cm_ll\n",
    "\n",
    "(cm_ll.iloc[0,0]+cm_ll.iloc[1,1])/len(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create level 2 (meta learner model) input data from outputs of the other two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "# XGB_features= 1- DataFrame(results_XGB)\n",
    "# XGB_features=pd.concat([XGB_features, DataFrame(results_XGB)], axis=1)\n",
    "def dataset_feature(input):\n",
    "    feature=1-DataFrame(input)\n",
    "    features=pd.concat([feature, DataFrame(input)], axis=1)\n",
    "    return features\n",
    "\n",
    "    \n",
    "XGB_features=dataset_feature(results_XGB)    \n",
    "#print(XGB_features)\n",
    "LL_features=dataset_feature(results_LL)\n",
    "\n",
    "MetaLearner_data=pd.concat([XGB_features, LL_features], axis=1)\n",
    "print(MetaLearner_data)\n",
    "MetaLearner_data.dtypes\n",
    "MetaLearner_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data back onto S3 for training the Meta learner- we will use a Ada boost from SKlearn library in cross validated format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test=pd.Series(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([Y_test, MetaLearner_data], axis=1).to_csv('meta_train.csv', index=False, header=False)\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join('metalearner/train/meta_train.csv')).upload_file('meta_train.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define environmnet parameters passed on to our training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_s3_uri = 's3://{}/metalearner/train/meta_train.csv'.format(bucket)\n",
    "model_dir='s3://{}/output/'.format(bucket)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Meta learner and add it to the endpoint we'd had already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "hyperparameters = {\"max_depth\":10, \"K\": 5}\n",
    "\n",
    "train_instance_type = \"ml.c5.xlarge\"\n",
    "inputs = {\"train\": csv_s3_uri}\n",
    "\n",
    "\n",
    "\n",
    "estimator_parameters = {\n",
    "    \"entry_point\": \"randomforest.py\",\n",
    "    \"dependencies\": [\"my_custom_library\"],\n",
    "    \"instance_type\": train_instance_type,\n",
    "    \"instance_count\": 1,\n",
    "    \"hyperparameters\": hyperparameters,\n",
    "    \"role\": role,\n",
    "    \"base_job_name\": \"randomforest-model\",\n",
    "    \"framework_version\": \"0.23-1\",\n",
    "    \"py_version\": \"py3\",\n",
    "    \"output_path\": model_dir\n",
    "}\n",
    "\n",
    "estimator = SKLearn(**estimator_parameters)\n",
    "estimator.fit(inputs)\n",
    "\n",
    "\n",
    "meta_learner_url=estimator.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I saw on githun some similar error suggesting to use the Attach function but this doesnt seem to help \n",
    "# #estimator = SKLearn.attach('training job name')\n",
    "# training_job_name = estimator.latest_training_job.name\n",
    "# training_job_name\n",
    "# estimator = SKLearn.attach(training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_3='783357654285.dkr.ecr.{}.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3'.format(region)\n",
    "\n",
    "container3 = { 'Image': container_3,\n",
    "               'ContainerHostname': 'SklearnContainer',\n",
    "                'ModelDataUrl': meta_learner_url}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create the new model and update the endpoint to include the meta learner as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm_client.create_model(ModelName = 'my-direct-modelML',\n",
    "              InferenceExecutionConfig = inferenceExecutionConfig,\n",
    "              ExecutionRoleArn = role,\n",
    "              Containers = [container1, container2, container3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = 'my-epc-ml',\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':        'ml.m4.xlarge',\n",
    "        'InitialInstanceCount': 2,\n",
    "        'InitialVariantWeight': 1,\n",
    "        'ModelName':            'my-direct-modelML',\n",
    "        'VariantName':          'AllTraffic'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm_client.update_endpoint(\n",
    "              EndpointName       = 'my-endpoint',\n",
    "              EndpointConfigName = 'my-epc-ml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test it on the entire test data\n",
    "meta_data=pd.read_csv(\"meta_train.csv\", header=None)\n",
    "meta_data=meta_data.iloc[: ,1:]\n",
    "meta_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test on the full test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# runtime_sm_client = boto3.Session().client('sagemaker-runtime')\n",
    "# import io\n",
    "# from io import StringIO\n",
    "# csv_file = io.StringIO()\n",
    "# results_ML=list()\n",
    "\n",
    "# for i in range(len(meta_data)):\n",
    "#     body=meta_data.iloc[[i]]\n",
    "#     csv_file=io.StringIO()\n",
    "#     body.to_csv(csv_file, sep=\",\", header=False, index=False)\n",
    "#     payload = csv_file.getvalue()\n",
    "#     response = runtime_sm_client.invoke_endpoint(EndpointName ='my-endpoint',ContentType = 'text/csv', TargetContainerHostname='thirdContainer', Body = payload)\n",
    "#     result = json.loads(response['Body'].read().decode())\n",
    "#     result=result['predictions'][0]['score']\n",
    "#     results_ML.append(result)\n",
    "# #print(results_LL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_ml=pd.crosstab(index=Y_test, columns=np.round(results_LL), rownames=['actuals'], colnames=['predictions'])\n",
    "cm_ml\n",
    "\n",
    "(cm_ml.iloc[0,0]+cm_ll.iloc[1,1])/len(Y_test)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-2:452832661640:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
